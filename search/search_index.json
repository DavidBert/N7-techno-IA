{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"AI technologies During this course you will: Learn about code versioning and containers to share your code and produce reproducible results Learn how to developp a recommender system Learn to train autonomous agents to play video games using Reinforcement Learning Learn to gain interpretability on your machine learning models Knowledge requirements Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning This course is inspired from the IA frameworks.","title":"Home"},{"location":"index.html#ai-technologies","text":"During this course you will: Learn about code versioning and containers to share your code and produce reproducible results Learn how to developp a recommender system Learn to train autonomous agents to play video games using Reinforcement Learning Learn to gain interpretability on your machine learning models","title":"AI technologies"},{"location":"index.html#knowledge-requirements","text":"Python Tutorial Elementary statistic tools Data Exploration and Clustering . Machine Learning High Dimensional & Deep Learning This course is inspired from the IA frameworks.","title":"Knowledge requirements"},{"location":"cloud_computing.html","text":"Development for Data Scientist: Cloud Computing Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Colab . Colab is normally used to run notebooks. Here, we are going to divert a little its functioning to use it as a cloud computing resource to run our scripts. You will have to: Write your Python scripts on your local machine Push your code to your Github repository Clone your repository to your Colab instance Run your code on Colab Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it! Practical session repository: If you haven't already done so, fork this repository and clone it on your computer. Data We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh The file data_utils.py contains some useful functions to load the dataset. Development in cloud computing environments Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. The network architecture We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.last_conv(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider $x$ a grayscaled image and $y$ its corresponding colored image. Training a parametrized network $f_\\theta$ to predict colorized images $\u0177$ amounts to minimizing the distance between the prediction $\u0177$ and the actual $y$. That is to say minimizing $MSE(y, f_\\theta(x))$. Fill the colorize.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Try to run your code on your local machine for one or two minibatches to check that everything is working. Training on cloud instances You now have everything to run your code on a cloud instance. Since we do not have access to free cloud computing ressources in this class, we will use Google Colab to run the code. Comit and push your code to your GitHub repository. Go to the following Google Colab notebook and modify the first cell to use your own GitHub repository. To enable the tensorboard compatibility with Colab and Pytorch you need to add the following lines in the colorize.py . import tensorflow as tf import tensorboard as tb tf.io.gfile = tb.compat.tensorflow_stub.io.gfile Then follow the notebook to run your code as a script. You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Once training is over, download your model's weights on your local machine. We will use them to create the web app. Web app with Gradio Complete the colorize_app.py file to create a web app that colorizes black and white images and run your application with the following command: python colorize_app.py --weights_path [path_to_the weights] You can test your app with random balck and white images from the net. For exemple one of these . Do you have any idea why the colors are so dull?","title":"Cloud Computing"},{"location":"cloud_computing.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"cloud_computing.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"cloud_computing.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Colab . Colab is normally used to run notebooks. Here, we are going to divert a little its functioning to use it as a cloud computing resource to run our scripts. You will have to: Write your Python scripts on your local machine Push your code to your Github repository Clone your repository to your Colab instance Run your code on Colab Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it!","title":"Practical Session"},{"location":"cloud_computing.html#practical-session-repository","text":"If you haven't already done so, fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"cloud_computing.html#data","text":"We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh The file data_utils.py contains some useful functions to load the dataset.","title":"Data"},{"location":"cloud_computing.html#development-in-cloud-computing-environments","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine.","title":"Development in cloud computing environments"},{"location":"cloud_computing.html#the-network-architecture","text":"We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.last_conv(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py","title":"The network architecture"},{"location":"cloud_computing.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider $x$ a grayscaled image and $y$ its corresponding colored image. Training a parametrized network $f_\\theta$ to predict colorized images $\u0177$ amounts to minimizing the distance between the prediction $\u0177$ and the actual $y$. That is to say minimizing $MSE(y, f_\\theta(x))$. Fill the colorize.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Try to run your code on your local machine for one or two minibatches to check that everything is working.","title":"Training script"},{"location":"cloud_computing.html#training-on-cloud-instances","text":"You now have everything to run your code on a cloud instance. Since we do not have access to free cloud computing ressources in this class, we will use Google Colab to run the code. Comit and push your code to your GitHub repository. Go to the following Google Colab notebook and modify the first cell to use your own GitHub repository. To enable the tensorboard compatibility with Colab and Pytorch you need to add the following lines in the colorize.py . import tensorflow as tf import tensorboard as tb tf.io.gfile = tb.compat.tensorflow_stub.io.gfile Then follow the notebook to run your code as a script. You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Once training is over, download your model's weights on your local machine. We will use them to create the web app.","title":"Training on cloud instances"},{"location":"cloud_computing.html#web-app-with-gradio","text":"Complete the colorize_app.py file to create a web app that colorizes black and white images and run your application with the following command: python colorize_app.py --weights_path [path_to_the weights] You can test your app with random balck and white images from the net. For exemple one of these . Do you have any idea why the colors are so dull?","title":"Web app with Gradio"},{"location":"dev.html","text":"Development for Data Scientist: Pytorch and Python Script Course Course notebook: Notebook Practical session For this session, you will have to write a script to train a small neural network on the MNIST dataset using Pytorch. During training, you will use tensorboard to: monitor your network across epochs manage your experiments and hyper-parameters provide some visualizations. The solution is available here. Try to complete the practical session without looking at it! Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. The network class: Using the figure above, fill in the following code, in the mnist_net.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x The training script The previous file contained our model class. We will now complete the training script train_mnist.py . This file will be used as a python script to train a neural network on the MNIST Dataset. The train() and test() methods are already implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from models import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total We will now implement the main method that will be called every time the python script is executed. We would like to give the user the possibility to adjust some parameters of the learning process, such as: The batch size The learning rate The number of training epochs To do so we will use Python argparse module . This module is used to write user-friendly command-line interfaces. Adding an argument to a python script using argaparse is pretty straightforward. First you need to import the argparse module and instanciate a parser within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments in the script by using an args variable. args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments and use the train method to train your network and the test method to compute the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Finally, save your model using the torch.save method. torch.save(net.state_dict(), 'mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train_mnist.py --epochs=5 --lr=1e-3 --batch_size=64 Monitoring and experiment management Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network Deploying your model with Gradio We will now create a simple application to guess the numbers drawn by a user from our saved model. We will use the Gradio library to quickly prototype machine learning applications and demonstrations with user friendly web interfaces. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs=gr.outputs.Label(num_top_classes=3), live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_app.py file so that the weights path is provided by the user and run your application with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST? Git Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Python Scripts"},{"location":"dev.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"dev.html#pytorch-and-python-script","text":"","title":"Pytorch and Python Script"},{"location":"dev.html#course","text":"","title":"Course"},{"location":"dev.html#course-notebook","text":"Notebook","title":"Course notebook:"},{"location":"dev.html#practical-session","text":"For this session, you will have to write a script to train a small neural network on the MNIST dataset using Pytorch. During training, you will use tensorboard to: monitor your network across epochs manage your experiments and hyper-parameters provide some visualizations. The solution is available here. Try to complete the practical session without looking at it!","title":"Practical session"},{"location":"dev.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"dev.html#the-network-class","text":"Using the figure above, fill in the following code, in the mnist_net.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x","title":"The network class:"},{"location":"dev.html#the-training-script","text":"The previous file contained our model class. We will now complete the training script train_mnist.py . This file will be used as a python script to train a neural network on the MNIST Dataset. The train() and test() methods are already implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from models import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total We will now implement the main method that will be called every time the python script is executed. We would like to give the user the possibility to adjust some parameters of the learning process, such as: The batch size The learning rate The number of training epochs To do so we will use Python argparse module . This module is used to write user-friendly command-line interfaces. Adding an argument to a python script using argaparse is pretty straightforward. First you need to import the argparse module and instanciate a parser within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments in the script by using an args variable. args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments and use the train method to train your network and the test method to compute the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Finally, save your model using the torch.save method. torch.save(net.state_dict(), 'mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train_mnist.py --epochs=5 --lr=1e-3 --batch_size=64","title":"The training script"},{"location":"dev.html#monitoring-and-experiment-management","text":"Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network","title":"Monitoring and experiment management"},{"location":"dev.html#deploying-your-model-with-gradio","text":"We will now create a simple application to guess the numbers drawn by a user from our saved model. We will use the Gradio library to quickly prototype machine learning applications and demonstrations with user friendly web interfaces. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs=gr.outputs.Label(num_top_classes=3), live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_app.py file so that the weights path is provided by the user and run your application with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST?","title":"Deploying your model with Gradio"},{"location":"dev.html#git","text":"Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Git"},{"location":"docker.html","text":"Development for Data Scientist: Docker Course Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder in your local machine. What can you see? Re-start your container and try to run your app in background mode. sudo docker exec -t container1 python ./devlop/colorize_app.py --weights_path ./devlop/unet.pth This is it for this session. Do not hesitate to play a little more with Docker. For instance try to train the mnist classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Introduction to Docker"},{"location":"docker.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docker.html#docker","text":"","title":"Docker"},{"location":"docker.html#course","text":"Slides","title":"Course"},{"location":"docker.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder in your local machine. What can you see? Re-start your container and try to run your app in background mode. sudo docker exec -t container1 python ./devlop/colorize_app.py --weights_path ./devlop/unet.pth This is it for this session. Do not hesitate to play a little more with Docker. For instance try to train the mnist classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Practical Session"},{"location":"evaluation.html","text":"Evaluation The evaluation is associated to the DEFI-IA Objective You will be evaluated on your capacity of acting like a Data Scientist , i.e. Handle a new dataset and explore it. Find a solution to address the defi's problem with a high score (above baseline). Explain the choosen algorithm. Write a complete pipeline to easily reproduce the results. Justify the choice of the algorithms and the environment (CPU/GPU, Cloud etc..). Share it and make your results easily reproducible (Git - docker, conda environment.). Notations Project - ( 60% ): a Git repository. The git should contain a clear markdown Readme, which describes ( 33% ) Which result you achieved? In which computation time? On which engine? What do I have to install to be able to reproduce the code? Which command do I have to run to reproduce the results? The code has to be easily reproducible. ( 33% ) Packages required has to be well described. (a requirements.txt files is the best) Conda command or docker command can be furnish The code should be clear and easily readable. ( 33% ) Final results can be run in a script and not a notebook. Only final code can be found in this script. Rapport - ( 40% ) 10 pages maximum: Quality of the presentation. 25% In-Deep explanation of the chosen algorithm. 25% Choice of the tools-infrastructure used. 25% Results you obtained. 25% Other details Group of 4 to 5 people (DEFI IA's team).","title":"Evaluation"},{"location":"evaluation.html#evaluation","text":"The evaluation is associated to the DEFI-IA","title":"Evaluation"},{"location":"evaluation.html#objective","text":"You will be evaluated on your capacity of acting like a Data Scientist , i.e. Handle a new dataset and explore it. Find a solution to address the defi's problem with a high score (above baseline). Explain the choosen algorithm. Write a complete pipeline to easily reproduce the results. Justify the choice of the algorithms and the environment (CPU/GPU, Cloud etc..). Share it and make your results easily reproducible (Git - docker, conda environment.).","title":"Objective"},{"location":"evaluation.html#notations","text":"Project - ( 60% ): a Git repository. The git should contain a clear markdown Readme, which describes ( 33% ) Which result you achieved? In which computation time? On which engine? What do I have to install to be able to reproduce the code? Which command do I have to run to reproduce the results? The code has to be easily reproducible. ( 33% ) Packages required has to be well described. (a requirements.txt files is the best) Conda command or docker command can be furnish The code should be clear and easily readable. ( 33% ) Final results can be run in a script and not a notebook. Only final code can be found in this script. Rapport - ( 40% ) 10 pages maximum: Quality of the presentation. 25% In-Deep explanation of the chosen algorithm. 25% Choice of the tools-infrastructure used. 25% Results you obtained. 25%","title":"Notations"},{"location":"evaluation.html#other-details","text":"Group of 4 to 5 people (DEFI IA's team).","title":"Other details"},{"location":"git_intro.html","text":"Introduction to git Slides","title":"Introduction to Git"},{"location":"git_intro.html#introduction-to-git","text":"Slides","title":"Introduction to git"},{"location":"interpretability.html","text":"Interpretability in Machine Learning Practical session Practical session Solution","title":"Interpretability in Machine Learning"},{"location":"interpretability.html#interpretability-in-machine-learning","text":"","title":"Interpretability in Machine Learning"},{"location":"interpretability.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"introduction.html","text":"Course Introduction:","title":"Course Introduction"},{"location":"introduction.html#course-introduction","text":"","title":"Course Introduction:"},{"location":"policy_gradient.html","text":"Introduction to Reinforcement Learning: Policy Gradient Slides Practical session","title":"Introduction to Reinforcement Learning:"},{"location":"policy_gradient.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"policy_gradient.html#policy-gradient","text":"Slides Practical session","title":"Policy Gradient"},{"location":"q_learning.html","text":"Introduction to Reinforcement Learning: From policy iteration to Deep Q-Learning Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"Introduction to Reinforcement Learning:"},{"location":"q_learning.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"q_learning.html#from-policy-iteration-to-deep-q-learning","text":"Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"From policy iteration to Deep Q-Learning"},{"location":"rec_sys.html","text":"Recommendation Systems Slides recommender systems Slides NLP Practical session Solution","title":"Recommender systems"},{"location":"rec_sys.html#recommendation-systems","text":"Slides recommender systems Slides NLP Practical session Solution","title":"Recommendation Systems"},{"location":"rl.html","text":"Introduction to Reinforcement Learning: Slides Intro Practical sessions: Policy iteration and Value Iteration: , solution: Q-learning: , solution: Deep Q-learning: , solution:","title":"Introduction to Reinforcement Learning"},{"location":"rl.html#introduction-to-reinforcement-learning","text":"Slides Intro Practical sessions: Policy iteration and Value Iteration: , solution: Q-learning: , solution: Deep Q-learning: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"schedule.html","text":"Schedule Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures. Session 1 Course Introduction : Course Introduction Development for Data Scientist : Introduction to Pytorch and Python scripts Introduction to Git : Introduction to Git Development for Data Scientist : Cloud Computing Development for Data Scientist : Introduction to Docker Session 2 Introduction to Reinforcement learning: Introduction to Reinforcement Learning Introduction to Reinforcement learning: Session 3 Recommender Systems : Introduction to Recommender systems Session 4 Explainable AI","title":"Schedule"},{"location":"schedule.html#schedule","text":"Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures.","title":"Schedule"},{"location":"schedule.html#session-1","text":"Course Introduction : Course Introduction Development for Data Scientist : Introduction to Pytorch and Python scripts Introduction to Git : Introduction to Git Development for Data Scientist : Cloud Computing Development for Data Scientist : Introduction to Docker","title":"Session 1"},{"location":"schedule.html#session-2","text":"Introduction to Reinforcement learning: Introduction to Reinforcement Learning Introduction to Reinforcement learning:","title":"Session 2"},{"location":"schedule.html#session-3","text":"Recommender Systems : Introduction to Recommender systems","title":"Session 3"},{"location":"schedule.html#session-4","text":"Explainable AI","title":"Session 4"},{"location":"text1.html","text":"Text Cleaning and Text Vectorization Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"text1.html#text-cleaning-and-text-vectorization","text":"Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"text2.html","text":"Words Embedding Slides Practical session","title":"Words Embedding"},{"location":"text2.html#words-embedding","text":"Slides Practical session","title":"Words Embedding"},{"location":"text3.html","text":"Text Recurrent Network Slides Practical session","title":"Text Recurrent Network"},{"location":"text3.html#text-recurrent-network","text":"Slides Practical session","title":"Text Recurrent Network"}]}